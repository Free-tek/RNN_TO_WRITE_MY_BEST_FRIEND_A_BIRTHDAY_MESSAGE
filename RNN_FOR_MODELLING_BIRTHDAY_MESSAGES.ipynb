{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_FOR_MODELLING_BIRTHDAY MESSAGES.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhLHXdaMgD8B",
        "colab_type": "text"
      },
      "source": [
        "Its my friends birthday tomorrow and it regular custom that we write codes to wish each other happy birthda, so i swept my phone for all the birthday test messages i have sent people and i also checked the internet too. So i am going to build a RNN to generate him a birthday message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlcnr-xOYWwg",
        "colab_type": "code",
        "outputId": "36bbaed2-fd92-4563-8378-1055c14a0691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "!pip install -q -U tensorflow>=2\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: tensorboard 2.0.1 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y-71q8GaLle",
        "colab_type": "text"
      },
      "source": [
        "Read in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDOizYvwZgGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open(\"birthday_messages.txt\", \"rb\").read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu2Kb7v3aIBj",
        "colab_type": "code",
        "outputId": "8fd7624f-ca2e-4f5c-a545-a5e81ae113db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print ('Length of text: {} characters'.format(len(corpus)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 226256 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SueFLkaTaWSa",
        "colab_type": "code",
        "outputId": "6c88fa96-fae4-4e5e-e21e-cd5f45d9fac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(corpus[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Happy birthday to the best sister in d world, I know you know that's a lie. I pray that you grow in d knowledgeable of Gods word and continue to prosper. I love you, I know you know dats another lie\n",
            "\n",
            "\n",
            "--------------------------\n",
            "\n",
            "Happy birthday to you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezuGJh1Bb4Ey",
        "colab_type": "code",
        "outputId": "76eb779b-a821-4471-cb9a-8dbaf42ddc28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(corpus))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135W-t1XdsTZ",
        "colab_type": "text"
      },
      "source": [
        "Vectorize text- map all unique characters to an integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LJ9rDgDdxwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "import numpy as np\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in corpus])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koYuLG-Id6QW",
        "colab_type": "code",
        "outputId": "4ffae6f0-bf62-4166-c3b3-4075bdc88a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  \"'\" :   4,\n",
            "  '(' :   5,\n",
            "  ')' :   6,\n",
            "  '*' :   7,\n",
            "  ',' :   8,\n",
            "  '-' :   9,\n",
            "  '.' :  10,\n",
            "  '/' :  11,\n",
            "  '0' :  12,\n",
            "  '1' :  13,\n",
            "  '2' :  14,\n",
            "  '3' :  15,\n",
            "  '5' :  16,\n",
            "  '6' :  17,\n",
            "  '9' :  18,\n",
            "  ':' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0shl94DeDVL",
        "colab_type": "code",
        "outputId": "3de71535-556c-42a9-e781-161a192ee2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(corpus[:25]), text_as_int[:25]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Happy birthday to the bes' ---- characters mapped to int ---- > [29 48 63 63 72  1 49 56 65 67 55 51 48 72  1 67 62  1 67 55 52  1 49 52\n",
            " 66]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBBAFyuieKV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 50\n",
        "examples_per_epoch = len(corpus)//(seq_length+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htu3kiLNgiLi",
        "colab_type": "code",
        "outputId": "e647395e-dff9-4c96-9235-2fd110328def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#convert stream of text into character indices\n",
        "# Create training examples / targets\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "H\n",
            "a\n",
            "p\n",
            "p\n",
            "y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpyy8YhvkYjX",
        "colab_type": "text"
      },
      "source": [
        "Divide characters into batch size of specific lenght"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgfZIbxugzQB",
        "colab_type": "code",
        "outputId": "a682b8f3-f6b7-4750-df41-ffd7b5bc7e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Happy birthday to the best sister in d world, I kno'\n",
            "\"w you know that's a lie. I pray that you grow in d \"\n",
            "'knowledgeable of Gods word and continue to prosper.'\n",
            "' I love you, I know you know dats another lie\\n\\n\\n---'\n",
            "'-----------------------\\n\\nHappy birthday to you frie'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juVPncLVmxK2",
        "colab_type": "text"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt0yygymlzKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLbEaQ8Vm72J",
        "colab_type": "text"
      },
      "source": [
        "Print the first examples input and target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2uUHSdqm8Zh",
        "colab_type": "code",
        "outputId": "2ca1ddc0-7478-461e-8d32-564f9176165d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'Happy birthday to the best sister in d world, I kn'\n",
            "Target data: 'appy birthday to the best sister in d world, I kno'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhwUbpJXn9yn",
        "colab_type": "text"
      },
      "source": [
        "The input of the RNN will start from '<' and predict '!' in the next time step till the final predicted output is 't' the last character in the sequence before it moves to the next sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGg-pcedsFns",
        "colab_type": "code",
        "outputId": "c87fa918-4f08-4582-9b92-609e58c1e58f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 29 ('H')\n",
            "  expected output: 48 ('a')\n",
            "Step    1\n",
            "  input: 48 ('a')\n",
            "  expected output: 63 ('p')\n",
            "Step    2\n",
            "  input: 63 ('p')\n",
            "  expected output: 63 ('p')\n",
            "Step    3\n",
            "  input: 63 ('p')\n",
            "  expected output: 72 ('y')\n",
            "Step    4\n",
            "  input: 72 ('y')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hotbjJ7fv1NY",
        "colab_type": "code",
        "outputId": "acbacd02-4cf3-4520-b8cd-c7a3193a1dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 82\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((82, 50), (82, 50)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diyWFjQnv1ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Batch size is usually the size of your vocabulary because you will want to maintain a reasonable proportional embedding distance between your vocab\n",
        "\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY8Aig6U4Ls4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdJPZIYW4RiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrALEylv4gzf",
        "colab_type": "code",
        "outputId": "ed692c41-6e02-4bc9-9f5f-3ff1cb18d20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(82, 50, 83) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPt6pdoJ6d7o",
        "colab_type": "code",
        "outputId": "f92c156e-8e6c-4c4b-83a6-cab09b22e980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (82, None, 256)           21248     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (82, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (82, None, 83)            85075     \n",
            "=================================================================\n",
            "Total params: 4,044,627\n",
            "Trainable params: 4,044,627\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfq7PJa97pc9",
        "colab_type": "text"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5nFttwc7rBg",
        "colab_type": "code",
        "outputId": "b7c136cc-1c5d-403d-ed43-d3a8a911a994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([51, 17, 80, 39,  2, 15, 37, 70, 12, 10, 26, 27, 41, 54,  5, 72, 30,\n",
              "       50, 44, 57, 69, 14, 20, 67, 23, 20,  5, 65, 33, 69, 59, 74,  1, 48,\n",
              "       40, 23, 43, 51, 51, 23, 61, 62, 48, 13, 74, 26,  9, 62, 20, 52])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE2glKqk8l5p",
        "colab_type": "text"
      },
      "source": [
        "Checking the sample text generated by this untrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD5YeymL765X",
        "colab_type": "code",
        "outputId": "17e6b2a0-ca6f-4d42-ad61-bfbd16f84d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " '--------------------------------------------------'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'd6“R!3Pw0.EFTg(yIcWjv2;tB;(rLvl{ aSBVddBnoa1{E-o;e'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M1ooepH87Tq",
        "colab_type": "text"
      },
      "source": [
        "Attach an optimizer, and a loss function\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the from_logits flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcFIrNua8s0a",
        "colab_type": "code",
        "outputId": "388803e7-211d-43b8-b8d6-fb110d1e1754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (82, 50, 83)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4240646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkkvXbaX_wN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClVv7IPhDUqo",
        "colab_type": "text"
      },
      "source": [
        "Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek1oJajA_xB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaegGaZ3FVeU",
        "colab_type": "code",
        "outputId": "c0cdb577-5972-4497-83e2-ffe09c7bca83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "54/54 [==============================] - 169s 3s/step - loss: 3.6424\n",
            "Epoch 2/20\n",
            "54/54 [==============================] - 166s 3s/step - loss: 2.4401\n",
            "Epoch 3/20\n",
            "54/54 [==============================] - 164s 3s/step - loss: 2.0410\n",
            "Epoch 4/20\n",
            "54/54 [==============================] - 163s 3s/step - loss: 1.8702\n",
            "Epoch 5/20\n",
            "54/54 [==============================] - 165s 3s/step - loss: 1.7523\n",
            "Epoch 6/20\n",
            "54/54 [==============================] - 165s 3s/step - loss: 1.6392\n",
            "Epoch 7/20\n",
            "54/54 [==============================] - 166s 3s/step - loss: 1.5399\n",
            "Epoch 8/20\n",
            "54/54 [==============================] - 166s 3s/step - loss: 1.4576\n",
            "Epoch 9/20\n",
            "54/54 [==============================] - 165s 3s/step - loss: 1.3777\n",
            "Epoch 10/20\n",
            "54/54 [==============================] - 165s 3s/step - loss: 1.3069\n",
            "Epoch 11/20\n",
            "54/54 [==============================] - 164s 3s/step - loss: 1.2414\n",
            "Epoch 12/20\n",
            "54/54 [==============================] - 166s 3s/step - loss: 1.1763\n",
            "Epoch 13/20\n",
            "54/54 [==============================] - 164s 3s/step - loss: 1.1174\n",
            "Epoch 14/20\n",
            "54/54 [==============================] - 173s 3s/step - loss: 1.0592\n",
            "Epoch 15/20\n",
            "54/54 [==============================] - 170s 3s/step - loss: 1.0053\n",
            "Epoch 16/20\n",
            "54/54 [==============================] - 169s 3s/step - loss: 0.9583\n",
            "Epoch 17/20\n",
            "54/54 [==============================] - 169s 3s/step - loss: 0.9080\n",
            "Epoch 18/20\n",
            "54/54 [==============================] - 170s 3s/step - loss: 0.8630\n",
            "Epoch 19/20\n",
            "54/54 [==============================] - 170s 3s/step - loss: 0.8218\n",
            "Epoch 20/20\n",
            "54/54 [==============================] - 170s 3s/step - loss: 0.7801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN-dX1FpjZz7",
        "colab_type": "code",
        "outputId": "4b75cb77-0f5f-450f-94d0-02041065dc29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_20'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhZNnkokE43z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3vUrZ-1QgzH",
        "colab_type": "code",
        "outputId": "e948f3fc-3d56-4259-a009-af0933721f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            21248     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 83)             85075     \n",
            "=================================================================\n",
            "Total params: 4,044,627\n",
            "Trainable params: 4,044,627\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBoM5iEDQy1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 0.2\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3KL_pKSREVY",
        "colab_type": "code",
        "outputId": "e663bf67-488a-43f2-ea70-d3f87bd049c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"Happy birthday \"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Happy birthday to my best friend that you simply have got a shock in the foremost of your distinctive day of yours be some person I hope you have got a good day.\n",
            "- - - - - - -\n",
            "“I hope you have got a good day.\n",
            "\n",
            "- - - - - - -\n",
            "“I hope you have got a good day.\n",
            "- - - - - - -\n",
            "“I hope you have got a good day. \n",
            "\n",
            "- \n",
            "\n",
            "\"I trust you have got a good day.\n",
            "\n",
            "- - - - - - -\n",
            "“Happy Birthday to my best pal! I trust you have got a good day.\n",
            "\n",
            "- - - - - - -\n",
            "“I hope you have got a good day. Hope your birthday is brimful with fun and laughter. Have a beautiful birthday to my best pal! I trust you have got a good day.\n",
            "- - - - - - -\n",
            "“I hope you have got a good day.\n",
            "\n",
            "- - - - - - -\n",
            "“I hope you have got a good day.\n",
            "\n",
            "-\n",
            "\n",
            "\"I trust you have got a good day.\n",
            "\n",
            "- - - - - - -\n",
            "“I hope you have got a good day. Hope your birthday is as astounding one for a prosperous years and see an opportune to recollect you have got a good day. \n",
            "\n",
            "- - - - - - - - -\n",
            "“I hope you have got a good day.\n",
            "\n",
            "- - - - - - -\n",
            "“I hope you have got a good day.\n",
            "\n",
            "-\n",
            "\n",
            "\"I trus\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}